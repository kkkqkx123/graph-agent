import { injectable, inject } from 'inversify';
import { ILLMClient } from '../../../../domain/llm/interfaces/llm-client.interface';
import { LLMRequest } from '../../../../domain/llm/entities/llm-request';
import { LLMResponse } from '../../../../domain/llm/entities/llm-response';
import { ModelConfig } from '../../../../domain/llm/value-objects/model-config';
import { TokenCalculator } from '../utils/token-calculator';

@injectable()
export class MockClient implements ILLMClient {
  private responses: Map<string, string> = new Map();

  constructor(
    @inject('TokenCalculator') private tokenCalculator: TokenCalculator,
    @inject('ConfigManager') private configManager: any
  ) {
    // Initialize some default mock responses
    this.setupDefaultResponses();
  }

  async generateResponse(request: LLMRequest): Promise<LLMResponse> {
    // Simulate processing delay
    await this.delay(100 + Math.random() * 200);

    try {
      // Generate mock response based on the last user message
      const lastUserMessage = request.messages
        .filter(msg => msg.role === 'user')
        .pop();

      let responseText = this.responses.get('default');
      
      if (lastUserMessage) {
        // Try to find a specific response based on keywords
        const message = lastUserMessage.content.toLowerCase();
        
        if (message.includes('hello') || message.includes('hi')) {
          responseText = this.responses.get('greeting');
        } else if (message.includes('help')) {
          responseText = this.responses.get('help');
        } else if (message.includes('weather')) {
          responseText = this.responses.get('weather');
        } else if (message.includes('code') || message.includes('programming')) {
          responseText = this.responses.get('code');
        }
      }

      // Calculate mock token usage
      const promptTokens = await this.calculateTokens(request);
      const completionTokens = Math.floor(responseText.length / 4);

      return LLMResponse.create(
        request.requestId,
        request.model,
        [{
          index: 0,
          message: {
            role: 'assistant',
            content: responseText
          },
          finish_reason: 'stop'
        }],
        {
          promptTokens,
          completionTokens,
          totalTokens: promptTokens + completionTokens
        },
        'stop',
        0 // duration - would need to be calculated
      );
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      throw new Error(`Mock client error: ${errorMessage}`);
    }
  }

  async calculateTokens(request: LLMRequest): Promise<number> {
    return this.tokenCalculator.calculateTokens(request);
  }

  async calculateCost(request: LLMRequest, response: LLMResponse): Promise<number> {
    const modelConfig = this.getModelConfig(request.model);
    const promptTokens = await this.calculateTokens(request);
    const completionTokens = response.usage?.completionTokens || 0;
    
    return (promptTokens * modelConfig.getPromptCostPer1KTokens() +
            completionTokens * modelConfig.getCompletionCostPer1KTokens()) / 1000;
  }

  setMockResponse(key: string, response: string): void {
    this.responses.set(key, response);
  }

  private setupDefaultResponses(): void {
    this.responses.set('default', 'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.');
    this.responses.set('greeting', 'Hello! I\'m a mock LLM client. How can I help you today?');
    this.responses.set('help', 'I can help you with various tasks. Since I\'m a mock client, I\'ll provide predefined responses for testing purposes.');
    this.responses.set('weather', 'I\'m sorry, but as a mock client, I cannot provide real weather information. Please integrate with a real weather service for accurate data.');
    this.responses.set('code', 'Here\'s a simple code example:\n\n```javascript\nfunction hello() {\n  console.log("Hello, World!");\n}\n```\n\nThis is just a mock response for testing purposes.');
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  private getModelConfig(model: string): ModelConfig {
    const configs = this.configManager.get('llm.mock.models', {});
    const config = configs[model];
    
    if (!config) {
      // Return default mock configuration
      return ModelConfig.create({
        model,
        provider: 'mock',
        maxTokens: 4096,
        contextWindow: 8192,
        temperature: 0.7,
        topP: 1.0,
        frequencyPenalty: 0.0,
        presencePenalty: 0.0,
        costPer1KTokens: {
          prompt: 0.0001, // $0.0001 per 1K prompt tokens
          completion: 0.0002 // $0.0002 per 1K completion tokens
        },
        supportsStreaming: true,
        supportsTools: true,
        supportsImages: false,
        supportsAudio: false,
        supportsVideo: false,
        metadata: {}
      });
    }

    return ModelConfig.create({
      model,
      provider: 'mock',
      maxTokens: config.maxTokens || 4096,
      contextWindow: 8192,
      temperature: config.temperature || 0.7,
      topP: config.topP || 1.0,
      frequencyPenalty: config.frequencyPenalty || 0.0,
      presencePenalty: config.presencePenalty || 0.0,
      costPer1KTokens: {
        prompt: config.promptTokenPrice || 0.0001,
        completion: config.completionTokenPrice || 0.0002
      },
      supportsStreaming: config.supportsStreaming ?? true,
      supportsTools: config.supportsTools ?? true,
      supportsImages: config.supportsImages ?? false,
      supportsAudio: config.supportsAudio ?? false,
      supportsVideo: config.supportsVideo ?? false,
      metadata: config.metadata || {}
    });
  }
}