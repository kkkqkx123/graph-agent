import { injectable, inject } from 'inversify';
import { ILLMClient } from '../../../../domain/llm/interfaces/llm-client.interface';
import { LLMRequest } from '../../../../domain/llm/entities/llm-request';
import { LLMResponse } from '../../../../domain/llm/entities/llm-response';
import { ModelConfig } from '../../../../domain/llm/value-objects/model-config';
import { TokenCalculator } from '../utils/token-calculator';

@injectable()
export class MockClient implements ILLMClient {
  private responses: Map<string, string> = new Map();

  constructor(
    @inject('TokenCalculator') private tokenCalculator: TokenCalculator,
    @inject('ConfigManager') private configManager: any
  ) {
    // Initialize some default mock responses
    this.setupDefaultResponses();
  }

  async generateResponse(request: LLMRequest): Promise<LLMResponse> {
    // Simulate processing delay
    await this.delay(100 + Math.random() * 200);

    try {
      // Generate mock response based on the last user message
      const lastUserMessage = request.messages
        .filter(msg => msg.role === 'user')
        .pop();

      let responseText = this.responses.get('default') || 'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.';
      
      if (lastUserMessage) {
        // Try to find a specific response based on keywords
        const message = lastUserMessage.content.toLowerCase();
        
        if (message.includes('hello') || message.includes('hi')) {
          responseText = this.responses.get('greeting') || 'Hello!';
        } else if (message.includes('help')) {
          responseText = this.responses.get('help') || 'How can I help you?';
        } else if (message.includes('weather')) {
          responseText = this.responses.get('weather') || 'I cannot provide weather information.';
        } else if (message.includes('code') || message.includes('programming')) {
          responseText = this.responses.get('code') || 'Here is a code example.';
        }
      }

      // Calculate mock token usage
      const promptTokens = await this.calculateTokens(request);
      const completionTokens = Math.floor(responseText.length / 4);

      return LLMResponse.create(
        request.requestId,
        request.model,
        [{
          index: 0,
          message: {
            role: 'assistant',
            content: responseText
          },
          finish_reason: 'stop'
        }],
        {
          promptTokens,
          completionTokens,
          totalTokens: promptTokens + completionTokens
        },
        'stop',
        0 // duration - would need to be calculated
      );
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      throw new Error(`Mock client error: ${errorMessage}`);
    }
  }

  async calculateTokens(request: LLMRequest): Promise<number> {
    return this.tokenCalculator.calculateTokens(request);
  }

  async calculateCost(request: LLMRequest, response: LLMResponse): Promise<number> {
    const modelConfig = this.getModelConfig();
    const promptTokens = await this.calculateTokens(request);
    const completionTokens = response.usage?.completionTokens || 0;
    
    return (promptTokens * modelConfig.getPromptCostPer1KTokens() +
            completionTokens * modelConfig.getCompletionCostPer1KTokens()) / 1000;
  }

  setMockResponse(key: string, response: string): void {
    this.responses.set(key, response);
  }

  private setupDefaultResponses(): void {
    this.responses.set('default', 'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.');
    this.responses.set('greeting', 'Hello! I\'m a mock LLM client. How can I help you today?');
    this.responses.set('help', 'I can help you with various tasks. Since I\'m a mock client, I\'ll provide predefined responses for testing purposes.');
    this.responses.set('weather', 'I\'m sorry, but as a mock client, I cannot provide real weather information. Please integrate with a real weather service for accurate data.');
    this.responses.set('code', 'Here\'s a simple code example:\n\n```javascript\nfunction hello() {\n  console.log("Hello, World!");\n}\n```\n\nThis is just a mock response for testing purposes.');
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  getModelConfig(): ModelConfig {
    const model = 'mock-model'; // 默认模型
    const configs = this.configManager.get('llm.mock.models', {});
    const config = configs[model];
    
    if (!config) {
      // Return default mock configuration
      return ModelConfig.create({
        model,
        provider: 'mock',
        maxTokens: 4096,
        contextWindow: 8192,
        temperature: 0.7,
        topP: 1.0,
        frequencyPenalty: 0.0,
        presencePenalty: 0.0,
        costPer1KTokens: {
          prompt: 0.0001, // $0.0001 per 1K prompt tokens
          completion: 0.0002 // $0.0002 per 1K completion tokens
        },
        supportsStreaming: true,
        supportsTools: true,
        supportsImages: false,
        supportsAudio: false,
        supportsVideo: false,
        metadata: {}
      });
    }

    return ModelConfig.create({
      model,
      provider: 'mock',
      maxTokens: config.maxTokens || 4096,
      contextWindow: 8192,
      temperature: config.temperature || 0.7,
      topP: config.topP || 1.0,
      frequencyPenalty: config.frequencyPenalty || 0.0,
      presencePenalty: config.presencePenalty || 0.0,
      costPer1KTokens: {
        prompt: config.promptTokenPrice || 0.0001,
        completion: config.completionTokenPrice || 0.0002
      },
      supportsStreaming: config.supportsStreaming ?? true,
      supportsTools: config.supportsTools ?? true,
      supportsImages: config.supportsImages ?? false,
      supportsAudio: config.supportsAudio ?? false,
      supportsVideo: config.supportsVideo ?? false,
      metadata: config.metadata || {}
    });
  }
  async generateResponseStream(request: LLMRequest): Promise<AsyncIterable<LLMResponse>> {
    // Mock streaming implementation
    const response = await this.generateResponse(request);
    
    async function* streamGenerator() {
      // Split the response into chunks to simulate streaming
      const content = response.choices[0]?.message?.content || '';
      const chunks = content.split(' ');
      
      for (const chunk of chunks) {
        yield LLMResponse.create(
          request.requestId,
          request.model,
          [{
            index: 0,
            delta: {
              content: chunk + ' '
            },
            finish_reason: null
          }],
          {
            promptTokens: response.usage?.promptTokens || 0,
            completionTokens: chunk.length / 4,
            totalTokens: (response.usage?.promptTokens || 0) + chunk.length / 4
          },
          null,
          0
        );
        
        // Simulate streaming delay
        await new Promise(resolve => setTimeout(resolve, 50));
      }
      
      // Send final chunk with finish_reason
      yield LLMResponse.create(
        request.requestId,
        request.model,
        [{
          index: 0,
          delta: {},
          finish_reason: 'stop'
        }],
        response.usage || { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        'stop',
        0
      );
    }
    
    return streamGenerator();
  }

  async isModelAvailable(model: string): Promise<boolean> {
    // Mock implementation - always return true for mock models
    return model.startsWith('mock-');
  }

  async getModelInfo(model: string): Promise<any> {
    // Mock implementation
    return {
      id: model,
      object: 'model',
      created: Date.now(),
      owned_by: 'mock-provider',
      permission: [],
      root: model,
      parent: null
    };
  }

  async validateRequest(request: LLMRequest): Promise<boolean> {
    // Mock implementation - basic validation
    return !!(request.messages && request.messages.length > 0);
  }

  async validateParameters(parameters: any): Promise<boolean> {
    // Mock implementation - always return true
    return true;
  }

  async preprocessRequest(request: LLMRequest): Promise<LLMRequest> {
    // Mock implementation - return request as-is
    return request;
  }

  async postprocessResponse(response: LLMResponse): Promise<LLMResponse> {
    // Mock implementation - return response as-is
    return response;
  }

  async healthCheck(): Promise<boolean> {
    // Mock implementation - always return true
    return true;
  }

  async getMetrics(): Promise<any> {
    // Mock implementation
    return {
      requestCount: 0,
      errorCount: 0,
      averageLatency: 0,
      lastRequestTime: null
    };
  }

  async resetMetrics(): Promise<void> {
    // Mock implementation - no-op
  }

  async close(): Promise<void> {
    // Mock implementation - no-op
  }

  async testConnection(): Promise<boolean> {
    // Mock implementation - always return true
    return true;
  }

  async getSupportedModels(): Promise<string[]> {
    // Mock implementation
    return ['mock-model', 'mock-model-turbo', 'mock-model-pro'];
  }

  async getModelCapabilities(model: string): Promise<any> {
    // Mock implementation
    return {
      supportsStreaming: true,
      supportsTools: true,
      supportsImages: false,
      supportsAudio: false,
      supportsVideo: false,
      maxTokens: 4096,
      contextWindow: 8192
    };
  }

  async estimateTokens(text: string): Promise<number> {
    // Mock implementation - rough estimate
    return Math.ceil(text.length / 4);
  }

  async truncateText(text: string, maxTokens: number): Promise<string> {
    // Mock implementation - simple truncation
    const estimatedTokens = await this.estimateTokens(text);
    if (estimatedTokens <= maxTokens) {
      return text;
    }
    
    // Rough truncation - not very accurate
    const ratio = maxTokens / estimatedTokens;
    return text.substring(0, Math.floor(text.length * ratio));
  }

  async formatMessages(messages: any[]): Promise<any[]> {
    // Mock implementation - return messages as-is
    return messages;
  }

  async parseResponse(response: any): Promise<LLMResponse> {
    // Mock implementation - return response as-is
    return response;
  }

  async handleError(error: any): Promise<LLMResponse> {
    // Mock implementation - create error response
    return LLMResponse.create(
      'error-request-id',
      'mock-model',
      [],
      { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
      'error',
      0,
      error instanceof Error ? error.message : String(error)
    );
  }
}