import { injectable, inject } from 'inversify';
import { ILLMClient } from '../../../../domain/llm/interfaces/llm-client.interface';
import { LLMRequest } from '../../../../domain/llm/entities/llm-request';
import { LLMResponse } from '../../../../domain/llm/entities/llm-response';
import { ModelConfig } from '../../../../domain/llm/value-objects/model-config';
import { TokenCalculator } from '../utils/token-calculator';

@injectable()
export class MockClient implements ILLMClient {
  private responses: Map<string, string> = new Map();

  constructor(
    @inject('TokenCalculator') private tokenCalculator: TokenCalculator,
    @inject('ConfigManager') private configManager: any
  ) {
    // Initialize some default mock responses
    this.setupDefaultResponses();
  }

  async generateResponse(request: LLMRequest): Promise<LLMResponse> {
    // Simulate processing delay
    await this.delay(100 + Math.random() * 200);

    try {
      // Generate mock response based on the last user message
      const lastUserMessage = request.messages
        .filter(msg => msg.role === 'user')
        .pop();

      let responseText = this.responses.get('default');
      
      if (lastUserMessage) {
        // Try to find a specific response based on keywords
        const message = lastUserMessage.content.toLowerCase();
        
        if (message.includes('hello') || message.includes('hi')) {
          responseText = this.responses.get('greeting');
        } else if (message.includes('help')) {
          responseText = this.responses.get('help');
        } else if (message.includes('weather')) {
          responseText = this.responses.get('weather');
        } else if (message.includes('code') || message.includes('programming')) {
          responseText = this.responses.get('code');
        }
      }

      // Calculate mock token usage
      const promptTokens = await this.calculateTokens(request);
      const completionTokens = Math.floor(responseText.length / 4);

      return new LLMResponse(
        request.id,
        responseText,
        {
          promptTokens,
          completionTokens,
          totalTokens: promptTokens + completionTokens
        },
        'stop',
        new Date()
      );
    } catch (error) {
      throw new Error(`Mock client error: ${error.message}`);
    }
  }

  async calculateTokens(request: LLMRequest): Promise<number> {
    return this.tokenCalculator.calculateTokens(request);
  }

  async calculateCost(request: LLMRequest, response: LLMResponse): Promise<number> {
    const modelConfig = this.getModelConfig(request.model);
    const promptTokens = await this.calculateTokens(request);
    const completionTokens = response.tokenUsage?.completionTokens || 0;
    
    return (promptTokens * modelConfig.promptTokenPrice + 
            completionTokens * modelConfig.completionTokenPrice) / 1000;
  }

  setMockResponse(key: string, response: string): void {
    this.responses.set(key, response);
  }

  private setupDefaultResponses(): void {
    this.responses.set('default', 'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.');
    this.responses.set('greeting', 'Hello! I\'m a mock LLM client. How can I help you today?');
    this.responses.set('help', 'I can help you with various tasks. Since I\'m a mock client, I\'ll provide predefined responses for testing purposes.');
    this.responses.set('weather', 'I\'m sorry, but as a mock client, I cannot provide real weather information. Please integrate with a real weather service for accurate data.');
    this.responses.set('code', 'Here\'s a simple code example:\n\n```javascript\nfunction hello() {\n  console.log("Hello, World!");\n}\n```\n\nThis is just a mock response for testing purposes.');
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  private getModelConfig(model: string): ModelConfig {
    const configs = this.configManager.get('llm.mock.models', {});
    const config = configs[model];
    
    if (!config) {
      // Return default mock configuration
      return new ModelConfig(
        model,
        0.0001, // $0.0001 per 1K prompt tokens
        0.0002, // $0.0002 per 1K completion tokens
        4096    // Max tokens
      );
    }

    return new ModelConfig(
      model,
      config.promptTokenPrice || 0.0001,
      config.completionTokenPrice || 0.0002,
      config.maxTokens || 4096
    );
  }
}