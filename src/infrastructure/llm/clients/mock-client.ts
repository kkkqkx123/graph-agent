import { injectable, inject } from 'inversify';
import { LLMRequest } from '../../../domain/llm/entities/llm-request';
import { LLMResponse } from '../../../domain/llm/entities/llm-response';
import { ModelConfig } from '../../../domain/llm/value-objects/model-config';
import { LLMMessage, LLMMessageRole } from '../../../domain/llm/value-objects/llm-message';
import { ID } from '../../../domain/common/value-objects/id';
import { BaseLLMClient } from './base-llm-client';
import { TokenCalculator } from '../token-calculators/token-calculator';
import { ProviderConfig, ApiType, ProviderConfigBuilder } from '../parameter-mappers';
import { MockParameterMapper } from '../parameter-mappers/providers/mock-parameter-mapper';
import { MockEndpointStrategy } from '../endpoint-strategies/providers/mock-endpoint-strategy';
import { BaseFeatureSupport } from '../parameter-mappers/interfaces/feature-support.interface';
import { LLM_DI_IDENTIFIERS } from '../di-identifiers';
import { HttpClient } from '../../common/http/http-client';
import { TokenBucketLimiter } from '../rate-limiters/token-bucket-limiter';
import { ConfigManager } from '../../config/config-manager';

@injectable()
export class MockClient extends BaseLLMClient {
  private responses: Map<string, string> = new Map();

  constructor(
    @inject(LLM_DI_IDENTIFIERS.HttpClient) httpClient: HttpClient,
    @inject(LLM_DI_IDENTIFIERS.TokenBucketLimiter) rateLimiter: TokenBucketLimiter,
    @inject(LLM_DI_IDENTIFIERS.TokenCalculator) tokenCalculator: TokenCalculator,
    @inject(LLM_DI_IDENTIFIERS.ConfigManager) configManager: ConfigManager
  ) {
    // 创建功能支持配置
    const featureSupport = new BaseFeatureSupport();
    featureSupport.supportsStreaming = true;
    featureSupport.supportsTools = true;
    featureSupport.supportsImages = false;
    featureSupport.supportsAudio = false;
    featureSupport.supportsVideo = false;
    featureSupport.supportsSystemMessages = true;
    featureSupport.supportsTemperature = true;
    featureSupport.supportsTopP = true;
    featureSupport.supportsMaxTokens = true;

    // 从配置中读取支持的模型列表
    const supportedModels = configManager.get('llm.mock.models', [
      'mock-model',
      'mock-model-turbo',
      'mock-model-pro'
    ]);

    // 创建提供商配置
    const providerConfig = new ProviderConfigBuilder()
      .name('Mock')
      .apiType(ApiType.CUSTOM)
      .baseURL('https://mock.api.example.com')
      .apiKey('mock-key')
      .endpointStrategy(new MockEndpointStrategy())
      .parameterMapper(new MockParameterMapper())
      .featureSupport(featureSupport)
      .defaultModel('mock-model')
      .supportedModels(supportedModels)
      .timeout(30000)
      .retryCount(3)
      .retryDelay(1000)
      .build();

    super(
      httpClient,
      rateLimiter,
      tokenCalculator,
      configManager,
      providerConfig
    );

    // Initialize some default mock responses
    this.setupDefaultResponses();
  }

  public override async generateResponse(request: LLMRequest): Promise<LLMResponse> {
    // Simulate processing delay
    await this.delay(100 + Math.random() * 200);

    try {
      return this.generateMockResponse(request);
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      throw new Error(`Mock client error: ${errorMessage}`);
    }
  }

  private generateMockResponse(request: LLMRequest): LLMResponse {
    // Generate mock response based on the last user message
    const lastUserMessage = request.messages
      .filter(msg => msg.isUser())
      .pop();

    let responseText = this.responses.get('default') || 'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.';

    if (lastUserMessage) {
      // Try to find a specific response based on keywords
      const message = lastUserMessage.getContent().toLowerCase();

      if (message.includes('hello') || message.includes('hi')) {
        responseText = this.responses.get('greeting') || 'Hello!';
      } else if (message.includes('help')) {
        responseText = this.responses.get('help') || 'How can I help you?';
      } else if (message.includes('weather')) {
        responseText = this.responses.get('weather') || 'I cannot provide weather information.';
      } else if (message.includes('code') || message.includes('programming')) {
        responseText = this.responses.get('code') || 'Here is a code example.';
      }
    }

    // Calculate mock token usage
    const promptTokens = Math.ceil(JSON.stringify(request.messages).length / 4);
    const completionTokens = Math.floor(responseText.length / 4);

    return LLMResponse.create(
      request.requestId,
      request.model,
      [{
        index: 0,
        message: LLMMessage.createAssistant(responseText),
        finish_reason: 'stop'
      }],
      {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens
      },
      'stop',
      0
    );
  }

  getSupportedModelsList(): string[] {
    // 使用配置中的模型列表，如果没有配置则返回空数组
    return this.providerConfig.supportedModels || [];
  }

  public getModelConfig(): ModelConfig {
    const model = 'mock-model'; // 默认模型
    const configs = this.configManager.get<Record<string, any>>('llm.mock.modelConfigs', {});
    const config = configs[model];

    if (!config) {
      // Return default mock configuration
      return ModelConfig.create({
        model,
        provider: 'mock',
        maxTokens: 4096,
        contextWindow: 8192,
        temperature: 0.7,
        topP: 1.0,
        frequencyPenalty: 0.0,
        presencePenalty: 0.0,
        costPer1KTokens: {
          prompt: 0.0001, // $0.0001 per 1K prompt tokens
          completion: 0.0002 // $0.0002 per 1K completion tokens
        },
        supportsStreaming: true,
        supportsTools: true,
        supportsImages: false,
        supportsAudio: false,
        supportsVideo: false,
        metadata: {}
      });
    }

    return ModelConfig.create({
      model,
      provider: 'mock',
      maxTokens: config.maxTokens || 4096,
      contextWindow: 8192,
      temperature: config.temperature || 0.7,
      topP: config.topP || 1.0,
      frequencyPenalty: config.frequencyPenalty || 0.0,
      presencePenalty: config.presencePenalty || 0.0,
      costPer1KTokens: {
        prompt: config.promptTokenPrice || 0.0001,
        completion: config.completionTokenPrice || 0.0002
      },
      supportsStreaming: config.supportsStreaming ?? true,
      supportsTools: config.supportsTools ?? true,
      supportsImages: config.supportsImages ?? false,
      supportsAudio: config.supportsAudio ?? false,
      supportsVideo: config.supportsVideo ?? false,
      metadata: config.metadata || {}
    });
  }

  public override async generateResponseStream(request: LLMRequest): Promise<AsyncIterable<LLMResponse>> {
    // Mock streaming implementation
    const response = await this.generateResponse(request);

    async function* streamGenerator() {
      // Split the response into chunks to simulate streaming
      const content = response.getFirstChoice()?.message.getContent() || '';
      const chunks = content.split(' ');

      for (const chunk of chunks) {
        yield LLMResponse.create(
          request.requestId,
          request.model,
          [{
            index: 0,
            message: LLMMessage.createAssistant(chunk + ' '),
            finish_reason: ''
          }],
          {
            promptTokens: response.usage?.promptTokens || 0,
            completionTokens: chunk.length / 4,
            totalTokens: (response.usage?.promptTokens || 0) + chunk.length / 4
          },
          '',
          0
        );

        // Simulate streaming delay
        await new Promise(resolve => setTimeout(resolve, 50));
      }

      // Send final chunk with finish_reason
      yield LLMResponse.create(
        request.requestId,
        request.model,
        [{
          index: 0,
          message: LLMMessage.createAssistant(''),
          finish_reason: 'stop'
        }],
        response.usage || { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        'stop',
        0
      );
    }

    return streamGenerator();
  }

  public override async isModelAvailable(model?: string): Promise<boolean> {
    // Mock implementation - always return true for mock models
    if (model) {
      return model.startsWith('mock-');
    }
    return true;
  }

  public override async getModelInfo(model?: string): Promise<any> {
    // Mock implementation
    return {
      id: model || 'mock-model',
      object: 'model',
      created: Date.now(),
      owned_by: 'mock-provider',
      permission: [],
      root: model || 'mock-model',
      parent: null
    };
  }

  public override async validateRequest(request: LLMRequest): Promise<{
    isValid: boolean;
    errors: string[];
    warnings: string[];
  }> {
    // Mock implementation - basic validation
    const isValid = !!(request.messages && request.messages.length > 0);
    return {
      isValid,
      errors: isValid ? [] : ['Messages are required'],
      warnings: []
    };
  }

  public override async validateParameters(parameters: any): Promise<boolean> {
    // Mock implementation - always return true
    return true;
  }

  public override async preprocessRequest(request: LLMRequest): Promise<LLMRequest> {
    // Mock implementation - return request as-is
    return request;
  }

  public override async postprocessResponse(response: LLMResponse): Promise<LLMResponse> {
    // Mock implementation - return response as-is
    return response;
  }

  public async getMetrics(): Promise<any> {
    // Mock implementation
    return {
      requestCount: 0,
      errorCount: 0,
      averageLatency: 0,
      lastRequestTime: null
    };
  }

  public async resetMetrics(): Promise<void> {
    // Mock implementation - no-op
  }

  public override async close(): Promise<boolean> {
    // Mock implementation - no-op
    return true;
  }

  public async testConnection(): Promise<boolean> {
    // Mock implementation - always return true
    return true;
  }

  public setMockResponse(key: string, response: string): void {
    this.responses.set(key, response);
  }

  private setupDefaultResponses(): void {
    this.responses.set('default', 'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.');
    this.responses.set('greeting', 'Hello! I\'m a mock LLM client. How can I help you today?');
    this.responses.set('help', 'I can help you with various tasks. Since I\'m a mock client, I\'ll provide predefined responses for testing purposes.');
    this.responses.set('weather', 'I\'m sorry, but as a mock client, I cannot provide real weather information. Please integrate with a real weather service for accurate data.');
    this.responses.set('code', 'Here\'s a simple code example:\n\n```javascript\nfunction hello() {\n  console.log("Hello, World!");\n}\n```\n\nThis is just a mock response for testing purposes.');
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}