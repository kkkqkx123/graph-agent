import { injectable, inject } from 'inversify';
import { LLMRequest } from '../../../domain/llm/entities/llm-request';
import { LLMResponse } from '../../../domain/llm/entities/llm-response';
import { ModelConfig } from '../../../domain/llm/value-objects/model-config';
import { LLMMessage, LLMMessageRole } from '../../../domain/llm/value-objects/llm-message';
import { BaseLLMClient } from './base-llm-client';
import { TokenCalculator } from '../token-calculators/token-calculator';
import { ProviderConfig, ApiType, ProviderConfigBuilder } from '../parameter-mappers';
import { MockParameterMapper } from '../parameter-mappers/mock-parameter-mapper';
import { MockEndpointStrategy } from '../endpoint-strategies/mock-endpoint-strategy';
import { BaseFeatureSupport } from '../parameter-mappers/interfaces/feature-support.interface';
import { TYPES } from '../../../di/service-keys';
import { HttpClient } from '../../../infrastructure/common/http/http-client';
import { TokenBucketLimiter } from '../rate-limiters/token-bucket-limiter';
import { getConfig } from '../../config/config';
import { MissingConfigurationError, InvalidConfigurationError, ExecutionError } from '../../../domain/common/exceptions';

@injectable()
export class MockClient extends BaseLLMClient {
  private responses: Map<string, string> = new Map();

  constructor(
    @inject(TYPES.HttpClient) httpClient: HttpClient,
    @inject(TYPES.TokenBucketLimiter) rateLimiter: TokenBucketLimiter,
    @inject(TYPES.TokenCalculator) tokenCalculator: TokenCalculator
  ) {
    // 创建功能支持配置
    const featureSupport = new BaseFeatureSupport();
    featureSupport.supportsStreaming = true;
    featureSupport.supportsTools = true;
    featureSupport.supportsImages = false;
    featureSupport.supportsAudio = false;
    featureSupport.supportsVideo = false;
    featureSupport.supportsSystemMessages = true;
    featureSupport.supportsTemperature = true;
    featureSupport.supportsTopP = true;
    featureSupport.supportsMaxTokens = true;

    // 从配置中读取必需的配置项
    const apiKey = getConfig().get('llm_runtime.mock.api_key');
    const defaultModel = getConfig().get('llm_runtime.mock.default_model');
    const supportedModels = getConfig().get('llm_runtime.mock.supported_models');

    // 验证必需配置
    if (!defaultModel) {
      throw new MissingConfigurationError('llm.mock.defaultModel');
    }
    if (!supportedModels || !Array.isArray(supportedModels) || supportedModels.length === 0) {
      throw new MissingConfigurationError('llm.mock.supportedModels');
    }

    // 创建提供商配置
    const providerConfig = new ProviderConfigBuilder()
      .name('Mock')
      .apiType(ApiType.CUSTOM)
      .baseURL('https://mock.api.example.com')
      .apiKey(apiKey)
      .endpointStrategy(new MockEndpointStrategy())
      .parameterMapper(new MockParameterMapper())
      .featureSupport(featureSupport)
      .defaultModel(defaultModel)
      .supportedModels(supportedModels)
      .timeout(30000)
      .retryCount(3)
      .retryDelay(1000)
      .build();

    super(httpClient, rateLimiter, tokenCalculator, providerConfig);

    // Initialize some default mock responses
    this.setupDefaultResponses();
  }

  public override async generateResponse(request: LLMRequest): Promise<LLMResponse> {
    // Simulate processing delay
    await this.delay(100 + Math.random() * 200);

    try {
      return await this.generateMockResponse(request);
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      throw new ExecutionError(`Mock client error: ${errorMessage}`);
    }
  }

  private async generateMockResponse(request: LLMRequest): Promise<LLMResponse> {
    // Generate mock response based on the last user message
    const lastUserMessage = request.messages.filter(msg => msg.isUser()).pop();

    let responseText =
      this.responses.get('default') ||
      'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.';

    if (lastUserMessage) {
      // Try to find a specific response based on keywords
      const message = lastUserMessage.getContent().toLowerCase();

      if (message.includes('hello') || message.includes('hi')) {
        responseText = this.responses.get('greeting') || 'Hello!';
      } else if (message.includes('help')) {
        responseText = this.responses.get('help') || 'How can I help you?';
      } else if (message.includes('weather')) {
        responseText = this.responses.get('weather') || 'I cannot provide weather information.';
      } else if (message.includes('code') || message.includes('programming')) {
        responseText = this.responses.get('code') || 'Here is a code example.';
      }
    }

    // Calculate mock token usage using unified token calculator
    const promptTokens = await this.tokenCalculator.calculateTokens(request);
    const completionTokens = await this.tokenCalculator.calculateTextTokens(responseText);

    return LLMResponse.create(
      request.requestId,
      request.model,
      [
        {
          index: 0,
          message: LLMMessage.createAssistant(responseText),
          finish_reason: 'stop',
        },
      ],
      {
        promptTokens,
        completionTokens,
        totalTokens: promptTokens + completionTokens,
      },
      'stop',
      0
    );
  }

  getSupportedModelsList(): string[] {
    if (!this.providerConfig.supportedModels) {
      throw new MissingConfigurationError('llm.mock.supportedModels');
    }
    return this.providerConfig.supportedModels;
  }

  public getModelConfig(): ModelConfig {
    const model = this.providerConfig.defaultModel;
    if (!model) {
      throw new MissingConfigurationError('llm.mock.defaultModel');
    }

    const configs = getConfig().get('llm_runtime.mock.models');
    const config = configs[model];

    if (!config) {
      throw new InvalidConfigurationError(model, `Mock模型配置未找到: ${model}。请在配置文件中提供该模型的完整配置。`);
    }

    // 验证必需的配置字段
    const requiredFields = [
      'maxTokens',
      'contextWindow',
      'temperature',
      'topP',
      'promptTokenPrice',
      'completionTokenPrice',
    ];
    for (const field of requiredFields) {
      if (config[field] === undefined || config[field] === null) {
        throw new InvalidConfigurationError(field, `Mock模型 ${model} 缺少必需配置字段: ${field}`);
      }
    }

    return ModelConfig.create({
      model,
      provider: 'mock',
      maxTokens: config.maxTokens,
      contextWindow: config.contextWindow,
      temperature: config.temperature,
      topP: config.topP,
      frequencyPenalty: config.frequencyPenalty ?? 0.0,
      presencePenalty: config.presencePenalty ?? 0.0,
      costPer1KTokens: {
        prompt: config.promptTokenPrice,
        completion: config.completionTokenPrice,
      },
      supportsStreaming: config.supportsStreaming ?? true,
      supportsTools: config.supportsTools ?? true,
      supportsImages: config.supportsImages ?? false,
      supportsAudio: config.supportsAudio ?? false,
      supportsVideo: config.supportsVideo ?? false,
      metadata: config.metadata ?? {},
    });
  }

  public override async generateResponseStream(
    request: LLMRequest
  ): Promise<AsyncIterable<LLMResponse>> {
    // Mock streaming implementation
    const response = await this.generateResponse(request);

    async function* streamGenerator() {
      // Split the response into chunks to simulate streaming
      const content = response.getFirstChoice()?.message.getContent() || '';
      const chunks = content.split(' ');

      for (const chunk of chunks) {
        // Calculate token count for this chunk
        // Note: Using simplified calculation for streaming performance
        // In production, consider using tokenCalculator.calculateTextTokens(chunk)
        const chunkTokens = Math.ceil(chunk.length / 4);

        yield LLMResponse.create(
          request.requestId,
          request.model,
          [
            {
              index: 0,
              message: LLMMessage.createAssistant(chunk + ' '),
              finish_reason: '',
            },
          ],
          {
            promptTokens: response.usage?.promptTokens || 0,
            completionTokens: chunkTokens,
            totalTokens: (response.usage?.promptTokens || 0) + chunkTokens,
          },
          '',
          0
        );

        // Simulate streaming delay
        await new Promise(resolve => setTimeout(resolve, 50));
      }

      // Send final chunk with finish_reason
      yield LLMResponse.create(
        request.requestId,
        request.model,
        [
          {
            index: 0,
            message: LLMMessage.createAssistant(''),
            finish_reason: 'stop',
          },
        ],
        response.usage || { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        'stop',
        0
      );
    }

    return streamGenerator();
  }

  protected override async parseStreamResponse(
    response: any,
    request: LLMRequest
  ): Promise<AsyncIterable<LLMResponse>> {
    // MockClient 完全覆盖了 generateResponseStream，这个方法不会被调用
    // 但为了满足抽象方法要求，提供一个实现
    throw new ExecutionError(
      'MockClient 不应该调用 parseStreamResponse，因为它完全覆盖了 generateResponseStream'
    );
  }

  public override async isModelAvailable(model?: string): Promise<boolean> {
    // Mock implementation - always return true for mock models
    if (model) {
      return model.startsWith('mock-');
    }
    return true;
  }

  public override async getModelInfo(model?: string): Promise<any> {
    // Mock implementation
    return {
      id: model || 'mock-model',
      object: 'model',
      created: Date.now(),
      owned_by: 'mock-provider',
      permission: [],
      root: model || 'mock-model',
      parent: null,
    };
  }

  public override async validateRequest(request: LLMRequest): Promise<{
    isValid: boolean;
    errors: string[];
    warnings: string[];
  }> {
    // Mock implementation - basic validation
    const isValid = !!(request.messages && request.messages.length > 0);
    return {
      isValid,
      errors: isValid ? [] : ['Messages are required'],
      warnings: [],
    };
  }

  public override async validateParameters(parameters: any): Promise<boolean> {
    // Mock implementation - always return true
    return true;
  }

  public override async preprocessRequest(request: LLMRequest): Promise<LLMRequest> {
    // Mock implementation - return request as-is
    return request;
  }

  public override async postprocessResponse(response: LLMResponse): Promise<LLMResponse> {
    // Mock implementation - return response as-is
    return response;
  }

  public async getMetrics(): Promise<any> {
    // Mock implementation
    return {
      requestCount: 0,
      errorCount: 0,
      averageLatency: 0,
      lastRequestTime: null,
    };
  }

  public async resetMetrics(): Promise<void> {
    // Mock implementation - no-op
  }

  public async testConnection(): Promise<boolean> {
    // Mock implementation - always return true
    return true;
  }

  public setMockResponse(key: string, response: string): void {
    this.responses.set(key, response);
  }

  private setupDefaultResponses(): void {
    this.responses.set(
      'default',
      'This is a mock response from the LLM client. In a real implementation, this would be generated by an actual language model.'
    );
    this.responses.set('greeting', "Hello! I'm a mock LLM client. How can I help you today?");
    this.responses.set(
      'help',
      "I can help you with various tasks. Since I'm a mock client, I'll provide predefined responses for testing purposes."
    );
    this.responses.set(
      'weather',
      "I'm sorry, but as a mock client, I cannot provide real weather information. Please integrate with a real weather service for accurate data."
    );
    this.responses.set(
      'code',
      'Here\'s a simple code example:\n\n```javascript\nfunction hello() {\n  console.log("Hello, World!");\n}\n```\n\nThis is just a mock response for testing purposes.'
    );
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
